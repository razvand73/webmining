{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03115d62",
   "metadata": {},
   "source": [
    "# Subiectul 1 – Scraping Metacritic\n",
    "Script Python care extrage, din 10 pagini Metacritic, următoarele câmpuri pentru fiecare film:\n",
    "- regizorul\n",
    "- scorul criticilor (**Metascore**)\n",
    "- scorul utilizatorilor (**User Score**)\n",
    "- durata filmului\n",
    "- titlul și URL‑ul paginii\n",
    "\n",
    "Rulează celula de mai jos după ce completezi lista `URLS` cu 10 link‑uri valide de film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f08a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time, requests, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/123.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# ⇣⇣⇣ Înlocuiește cu 10 URL‑uri de film Metacritic\n",
    "URLS = [\n",
    "    \"https://www.metacritic.com/movie/inside-out-2\",\n",
    "    \"https://www.metacritic.com/movie/dune-part-two\",\n",
    "    # … alte 8\n",
    "]\n",
    "\n",
    "def parse_movie(url: str) -> dict:\n",
    "    \"\"\"Returnează metadatele unui film.\"\"\"\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=10)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    title_tag = soup.find(\"h1\")\n",
    "    title = title_tag.get_text(strip=True) if title_tag else url.split(\"/\")[-1]\n",
    "\n",
    "    m_tag = soup.select_one(\"span.metascore_w.larger.movie\")         or soup.select_one(\"span.metascore_w.xlarge.movie\")\n",
    "    critic = int(m_tag.text.strip()) if m_tag else None\n",
    "\n",
    "    u_tag = soup.select_one(\"div.metascore_w.user\")\n",
    "    try:\n",
    "        user = float(u_tag.text.strip()) if u_tag else None\n",
    "    except ValueError:\n",
    "        user = None\n",
    "\n",
    "    run_tag = soup.find(\"li\", class_=\"runtime\")\n",
    "    runtime = run_tag.get_text(strip=True) if run_tag else None\n",
    "\n",
    "    dir_tag = soup.find(\"li\", class_=\"director\")\n",
    "    if dir_tag:\n",
    "        director = \" \".join(dir_tag.stripped_strings).replace(\"Director:\", \"\").strip()\n",
    "    else:\n",
    "        lbl = soup.find(\"span\", string=lambda t: t and \"Director\" in t)\n",
    "        director = lbl.find_next(\"span\").get_text(strip=True) if lbl else None\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"director\": director,\n",
    "        \"critic_score\": critic,\n",
    "        \"user_score\": user,\n",
    "        \"runtime\": runtime,\n",
    "        \"url\": url,\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "for link in URLS:\n",
    "    try:\n",
    "        rows.append(parse_movie(link))\n",
    "        print(\"✓\", link)\n",
    "    except Exception as exc:\n",
    "        print(\"✗\", link, exc)\n",
    "    time.sleep(2)  # politeness delay\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"metacritic_films.csv\", index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3fa068",
   "metadata": {},
   "source": [
    "# Subiectul 2 – Arbore de decizie (Congressional Voting Records)\n",
    "Se folosește setul de date **Congressional Voting Records**. Ultimele 10 % din rânduri devin set de test, iar restul – set de antrenare.\n",
    "Celula de mai jos descarcă datele (dacă nu există local), antrenează un arbore de decizie, afișează acuratețea și salvează o imagine cu arborele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b73c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd, matplotlib.pyplot as plt, urllib.request\n",
    "from pathlib import Path\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "DATA_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data\"\n",
    "LOCAL = Path(\"house-votes-84.data\")\n",
    "\n",
    "if not LOCAL.exists():\n",
    "    urllib.request.urlretrieve(DATA_URL, LOCAL)\n",
    "\n",
    "COLS = [\"class\"] + [f\"vote_{i}\" for i in range(16)]\n",
    "df = pd.read_csv(LOCAL, header=None, names=COLS)\n",
    "\n",
    "df.replace({\"y\": 1, \"n\": 0, \"?\": pd.NA}, inplace=True)\n",
    "df = df.dropna()\n",
    "print(f\"Rânduri după curățare: {len(df)}\")\n",
    "\n",
    "split = int(len(df) * 0.9)\n",
    "train, test = df.iloc[:split], df.iloc[split:]\n",
    "\n",
    "X_train, y_train = train.drop(\"class\", axis=1), train[\"class\"]\n",
    "X_test , y_test  = test.drop(\"class\", axis=1),  test[\"class\"]\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = tree.predict(X_test)\n",
    "print(\"Acuratețe:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# importanța trăsăturilor\n",
    "imp = pd.Series(tree.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "display(imp.to_frame(\"Feature importance\").head(10))\n",
    "\n",
    "# Vizualizează arborele\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(tree, feature_names=X_train.columns, class_names=tree.classes_, filled=True)\n",
    "plt.title(\"Decision Tree – Congressional Voting\")\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"voting_tree.png\", dpi=150, bbox_inches=\"tight\")\n",
    "print(\"Grafic salvat -> voting_tree.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64abe73",
   "metadata": {},
   "source": [
    "# Subiectul 3 – Clustering pe setul **Iris**\n",
    "Aplicăm `KMeans` cu *k = 3*, apoi evaluăm cât de bine corespund clusterele speciilor reale folosind **Adjusted Rand Index** și **Silhouette Score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782726cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "species_names = iris.target_names\n",
    "\n",
    "# Standardize\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, n_init=\"auto\", random_state=0)\n",
    "labels = kmeans.fit_predict(X_std)\n",
    "\n",
    "ari = adjusted_rand_score(y, labels)\n",
    "sil = silhouette_score(X_std, labels)\n",
    "\n",
    "print(f\"Adjusted Rand Index: {ari:.3f}\")\n",
    "print(f\"Silhouette Score   : {sil:.3f}\\n\")\n",
    "\n",
    "# tabel confuzie cluster vs specie\n",
    "cross = pd.crosstab(y, labels, rownames=[\"Specie reală\"], colnames=[\"Cluster\"])\n",
    "display(cross)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
